{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "from BBC_scrapper_amharic import BBCScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = 1\n",
    "csv_file_path = \"data/bbc_amharic.csv\"\n",
    "\n",
    "# Replace with the base URL of the page you want to scrape\n",
    "base_url = \"https://www.bbc.com/amharic/topics/c7zp57r92v5t\"\n",
    "prefix = \"https://www.bbc.com/amharic/articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to a csv file\n",
    "\n",
    "\n",
    "def save_to_csv(news_articles, csv_file_path):\n",
    "    # Save the articles to a csv file\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=news_articles[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(news_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bbc.com/amharic/topics/c7zp57r92v5t?page=1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(page_url)\n\u001b[1;32m     10\u001b[0m scrapper \u001b[38;5;241m=\u001b[39m BBCScraper(page_url, prefix)\n\u001b[0;32m---> 11\u001b[0m news_articles \u001b[38;5;241m=\u001b[39m \u001b[43mscrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m total_articles\u001b[38;5;241m.\u001b[39mextend(news_articles)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(news_articles)\n",
      "File \u001b[0;32m~/code/10Academy-training/week4/llm_datawarehouse/scrapper/BBC_scrapper_amharic.py:82\u001b[0m, in \u001b[0;36mBBCScraper.get_articles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_articles\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     81\u001b[0m   article_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_article_urls()\n\u001b[0;32m---> 82\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict(url) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m article_urls]\n",
      "File \u001b[0;32m~/code/10Academy-training/week4/llm_datawarehouse/scrapper/BBC_scrapper_amharic.py:82\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_articles\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     81\u001b[0m   article_urls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_article_urls()\n\u001b[0;32m---> 82\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m article_urls]\n",
      "File \u001b[0;32m~/code/10Academy-training/week4/llm_datawarehouse/scrapper/BBC_scrapper_amharic.py:66\u001b[0m, in \u001b[0;36mBBCScraper.to_dict\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, url):\n\u001b[0;32m---> 66\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_soup\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m: url,\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_title(soup),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_category(soup),\n\u001b[1;32m     78\u001b[0m     }\n",
      "File \u001b[0;32m~/code/10Academy-training/week4/llm_datawarehouse/scrapper/BBC_scrapper_amharic.py:14\u001b[0m, in \u001b[0;36mBBCScraper.get_soup\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_soup\u001b[39m(\u001b[38;5;28mself\u001b[39m, url):\n\u001b[1;32m     13\u001b[0m   response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m---> 14\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/10Academy-training/week4/llm_datawarehouse/.venv/lib/python3.9/site-packages/bs4/__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/code/10Academy-training/week4/llm_datawarehouse/.venv/lib/python3.9/site-packages/bs4/__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[0;32m~/code/10Academy-training/week4/llm_datawarehouse/.venv/lib/python3.9/site-packages/bs4/builder/_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    378\u001b[0m parser\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     parser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/html/parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/html/parser.py:170\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m, i):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen\u001b[38;5;241m.\u001b[39mmatch(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[1;32m    172\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[0;32m/usr/lib/python3.9/html/parser.py:321\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rest:\n\u001b[1;32m    320\u001b[0m     attrvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attrvalue[:\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m==\u001b[39m attrvalue[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m    322\u001b[0m      attrvalue[:\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m==\u001b[39m attrvalue[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    323\u001b[0m     attrvalue \u001b[38;5;241m=\u001b[39m attrvalue[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attrvalue:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_articles = []\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "for page_number in range(1, pages + 1):\n",
    "    page_url = f\"https://www.bbc.com/amharic/topics/c7zp57r92v5t?page={page_number}\"\n",
    "    print(page_url)\n",
    "\n",
    "    scrapper = BBCScraper(page_url, prefix)\n",
    "    news_articles = scrapper.get_articles()\n",
    "    total_articles.extend(news_articles)\n",
    "    print(news_articles)\n",
    "\n",
    "    # Save the articles to a csv file\n",
    "    save_to_csv(news_articles, csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.bbc.com/amharic/articles/crgywn202l0o',\n",
       " 'title': 'ፕሬዝዳንት ኢሳያስ የጠቀሱት “ሌላ የጦርነት አዙሪት” ማንን ያሰጋል? - BBC News አማርኛ',\n",
       " 'date': None,\n",
       " 'image_url': 'https://a1.api.bbc.co.uk/hit.xiti?s=598342&s2=4&p=amharic.articles.crgywn202l0o.page&x1=[urn%3Abbc%3Aoptimo%3Aasset%3Acrgywn202l0o]&x2=[responsive]&x3=[news-amharic]&x4=[am]&x7=[article]&x8=[simorgh-nojs]&x9=[%E1%8D%95%E1%88%AC%E1%8B%9D%E1%8B%B3%E1%8A%95%E1%89%B5%2520%E1%8A%A2%E1%88%B3%E1%8B%AB%E1%88%B5%2520%E1%8B%A8%E1%8C%A0%E1%89%80%E1%88%B1%E1%89%B5%2520%E2%80%9C%E1%88%8C%E1%88%8B%2520%E1%8B%A8%E1%8C%A6%E1%88%AD%E1%8A%90%E1%89%B5%2520%E1%8A%A0%E1%8B%99%E1%88%AA%E1%89%B5%E2%80%9D%2520%E1%88%9B%E1%8A%95%E1%8A%95%2520%E1%8B%AB%E1%88%B0%E1%8C%8B%E1%88%8D%3F]&x11=[2024-05-27T04%3A06%3A06.232Z]&x12=[2024-05-27T04%3A06%3A06.232Z]&x13=[Ethiopia%2Band%2BEritrea%2Bpeace%2Bagreement~Sudan~Politics~War~Eritrea~Ethiopia]&x14=[5a491dfd-e977-43df-a5c0-39a35501688f~620270e1-cd82-443d-a7e4-98d177627606~75612fa6-147c-4a43-97fa-fcf70d9cced3~9d4b0cb6-1f21-40f0-ac81-75f58f01f884~a6e771f0-8659-456a-9334-94a451fc81a6~e986aff5-6b26-4638-b468-371d1d9617b4]&x17=[Ethiopia%2Band%2BEritrea%2Bpeace%2Bagreement~Sudan~Politics~War~Eritrea~Ethiopia]',\n",
       " 'text': 'የፎቶው ባለመብት, @hawelti ኤርትራ የነጻነት በዓሏን ስታከበር እምብዛም በመገናኛ ብዙኃን የማይቀርቡት የፕሬዝዳንት ኢሳያስ አፈወርቂ ንግግር በብዙዎች ዘንድ ይጠበቃል። ለዚህም ምክንያቱ በንግግራቸው የመንግሥታቸውን አካባቢያዊ እና ዓለም አቀፋዊ አቋም እንዲሁም ወቅታዊ ጉዳዮችን በሚመለከት መልዕክት ስለሚያስተላልፉ ነው። በዘንድሮው 33ኛው የኤርትራ ነጻነት በዓል ዕለትም ፕሬዝዳንት ኢሳያስ እንደወትሮቸው ምዕራባውያንን አንስተው አብጠልጠለዋል፤ የፍልስጤማውያንንም ጉዳይ አንስተው የአገራቸውን አቋም አንጸባርቀዋል። በአፍሪካ ለረጅም ዘመናት በሥልጣን ላይ በመቆየት ከሚጠቀሱት መሪዎች መካከል አንዱ የሆኑት ፕሬዝዳንት ኢሳያስ፣ ኤርትራን ለነጻነት ያበቃውን አማጺ ቡድን ለሦስት አስርት ዓመታት ያህል ከመምራት ጀምሮ፣ ከነጻነት በኋላ ደግሞ ላለፉት 33 ዓመታት በፕሬዝዳንትነት ሥልጣን ላይ ይገኛሉ። ፕሬዝዳንት ኢሳያስ ባለፉት አስርት ዓመታት ኤርትራን ካለ ሕገ መንግሥት እና በሕዝብ የተመረጠ ምክር ቤት በፈላጭ ቆራጭነት ከመምራታቸው ባሻገር በሕዝባቸው ላይ ጥብቅ ቁጥጥር በማድረግ እና በመብት ጥሰቶች ይወቀሳሉ። ስትራተጂክ በሆነው ቀይ ባሕር ላይ የምትገኘው ኤርትራን የሚመሩት ኢሳያስ ከምዕራባውያን ጋር ሆድ እና ጀርባ ሲሆኑ ባገኙት አጋጣሚ ሁሉ ሲተቿቸው ይደመጣሉ። በአሁኑ ወቅትም ኤርትራ ከምሥራቃውያኑ ሩሲያ እና ቻይና ጋር ያላት ግንኙነት እየተጠናከረ ይገኛል። በኢትዮጵያ ከጠቅላይ ሚኒስትር ዐቢይ አሕመድ ወደ ሥልጣን መምጣት ጋር ተያይዞ ከ20 ዓመታት በላይ የተበላሸውን የሁለቱን አገራት ግንኙነት ለማሻሻል ችለው ነበር። በሰሜን ኢትዮጵያ ትግራይ ውስጥ በተካሄደው ጦርነት ውስጥም የኢትዮጵያ ሠራዊትን ሠራዊት ደግፈው ጦራቸውን ማሰማራታቸው ይታወሳል። ደም አፋሳሹ ጦርነት በኢትዮጵያ መንግሥት እና በህወሓት መካከል ደቡብ አፍሪካ ላይ በተደረሰ ስምምነት መቋጨቱን ተከትሎ በኢትዮጵያ እና በኤርትራ መካከል ያለው ግንኙነት ስላለበት ሁኔታ በእርግጠኝነት ለመናገር በሚያዳግት ሁኔታ ላይ ይገኛል። በተለይ ደግሞ የኢትዮጵያው ጠቅላይ ሚኒስትር አገራቸው በቀጠናው ካላት ከፍተኛ ሕዝብ አንጻር በቀይ ባሕር ፖለቲካ ውስጥ ሚና ሊኖራት እንደሚገባ በማውሳት የባሕር በር የማግኘት ሃሳባቸውን ያፋ ካደረጉ በኋላ በርካቶች የሁለቱ አገራት ግንኙነት እንደቀድሞው አይደለም ይላሉ። በኤርትራ በኩል በኢትዮጵያ ፍላጎት እንዲሁም በአጠቃላይ በቀጠናው ስላሉ ሁኔታዎች አልፎ አልፎ ከሚሰሙት ነገሮች ባሻገር በይፋ የሚባሉ ነገሮች አለመኖራቸው የፕሬዝዳንቱ ንግግርን ተጠባቂ ያደርገዋል። ለዚህ ደግሞ የአገሪቱ የነጻነት በዓል ዋነኛው መድረክ ነው። ፕሬዝዳንት ኢሳያስ ባለፈው ሳምንት ማብቂያ ላይ በተከበረው የነጻነት በዓል ላይ አካባቢያዊ እና ዓለም አቀፋዊ ጉዳዮችን አንስተዋል። ከዚህም ውስጥ በአካባቢው ጦርነት ለመቀስቀስ የሚፈልጉ ኃይሎች እንዳሉ መጥቀሳቸው በብዙዎች ዘንድ መነጋጋሪያ ሆኗል። ኤርትራ በዙሪያዋ ባሉ አገራት ውስጥ ግጭቶች እየተካሄዱ መሆናቸው እና ተጨማሪ የጦርነት ስጋት ስለመኖሩ መናጋራቸው የት እና እንዴት የሚል ጥያቄን ያስነሳል። ሱዳን በእስር በርስ ጦርነት አንድ ዓመት ኣሳልፋለች፣ ሰሜናዊው ኢትዮጵያም ከጦርነት ቢወጣም በቅጡ አላገገመም፣ ከቀይ ባሕር ማዶ ያለችው የመንም በዓመታት ጦርነት ውስጥ ናት። ታዲያ የኢሳያስ የሌላ ዙር ጦርነት ስጋት ከየት የመጣ ነው? ኢሳያስ “የበላይነት እና ሁሉን የመቆጣጠር” ካምፕ ያሉት ኃይል በአገራቸው ላይ ሲያካሂዱት የነበረው ጠላትነት አለመሳካቱ ከፍተኛ ብስጭት ፈጥሯል ብለዋል። “በዚህ ምክንያት በአሁኑ ወቅት ሌላ የጦርነት አዙሪት በመቀስቀስ ላይ ይገኛሉ። ይህም የአደባባይ ምሥጢር ሆኗል” በማለት ዝርዝሩ በጊዜ ሂደት ይፋ እንደሚሆን ተናግረዋል። ፕሬዝዳንቱ የጠላትነት ተግባርን ሲያከናውኑ ነበር ያለጨው እና ግጭቶችን በመጠንሰስ ላይ ናቸው ያሏቸውን “የበላይነት እና ሁሉን የመቆጣጠር” ካምፕ አባላት ማንነት ከመግለጽ ተቆጥበዋል። ፕሬዝዳንት ኢሳያስ “የበላይነት እና ሁሉን የመቆጣጠር” ኃይሎች ሲሉ “በአጠቃላይ ምዕራቡን ዓለም በተለይ ደግሞ አሜሪካንን ለመጥቀስ ነው” የሚሉት ዩናይትድ ኪንግደም የሚገኙት የአፍሪካ ቀንድ ፖለቲካ ተንታኝ አብዱራህማን ሰኢድ (ቦሐሺም) ናቸው። በኢሳያስ የምትመራው ኤርትራ ከዩናይትድ ስቴትስ ጋር ያላት ግንኙነት ከተበላሸ አስርታት የተቆጠሩ ሲሆን፣ እሳቸውም ባገኙት አጋጣሚ በአካባቢው በምትከተለው ፖሊሲ ምክንያት ሲከሷት እና ሰተቿት ቆይተዋል። አሜሪካንም በሰብአዊ መብት ይዞታዋ እና በቀጠናው ፖለቲካ ውስጥ አላት በምተለው አሉታዊ ሚና ምክንያት ኤርትራን ትወቅሳለች። ይህ የአቶ ኢሳያስ ክስ እውነታ ቢኖረውም እሳቸው እና አስተዳደራቸው በኤርትራ ማድረግ ያልቻሉትን የሚጠበቅባቸውን ሥራ ባለማከናወናቸው ከሕዝቡ ሊነሳ የሚችለውን ጥያቄ እና ቅሬታ ለመሸፋፈን ሌሎችን ተጠያቂ ማድረግ በፕሬዝዳንት ኢሳያስ በኩል  የተለመደ ነገር ነው ይላሉ አብዱራህማን። “ኢሳያስ በንግግራቸው የተለያዩ ወገኖችን በመክሰስ፣ ይህንንም ሰበብ አድርገው የኤርትራ ሕዝብን እና ያላቸውን የሕግ የበላይነት እና ዴሞክራሲያዊ አስተደደር ፍላጎትን እስከ መጨረሻው በቁጥጥራቸው ስር ማደረጋቸው የተለመደ ሆኗል።” አክለውም ዓለም በአንድ ወይም በሌላ ኃይል የበላይነት ተጽእኖ ሥር መሆኗ የነበረ እና አሁንም ያለ መሆኑ እውነት ቢሆንም፣ “ኢሳያስ ግን በዚህ ሁኔታ ውስጥ በዘላቂነት እራሷን ችላ የምትጓዝ አገር በመገንባት በኩል ይህ ነው የሚባል ሥራ አላከናወኑም” በማለት ይተቻሉ። የፎቶው ባለመብት, @hawelti ከጊዜ ወደ ጊዜ በጦርነት አዙሪት ውስጥ በቆየው የአፍሪካ ቀንድ ፕሬዝዳንት ኢሳያስ የበላይነት እና ሁሉን የመቆጣጠር ኃይል ያሏቸው ወገኖች ሌላ ዙር ጦርነት ለመቀስቀስ ፍላጎት እንዳላቸው መናገራቸው የቀጠናው አሳሳቢ ጉዳይ ነው። የአፍሪካ ቀንድ የፖለቲካ ተንታኙ አብዱራህማን ሰኢድ በአካባቢው ግጭት የማጋጠሙ ነገር የተዘጋ ባይሆንም የፕሬዝዳንቱ ንግግር ድክመታቸውን ለመሸፈን እና ሕዝቡን በስጋት ለመያዝ የሚጠቀሙበት የተለመደ ነገር ነው ይላሉ። “ባለፉት 33 ዓመታት ፕሬዝዳንቱ እራሳቸውን ከአንድ ወታደራዊ ቡድን መሪነት አገሩን በዴሞክረሲያዊ መንገድ ወደሚመራ ፖለቲከኛ ለማሸጋገር አለመቻላቸውን ለመሸፈን በየጊዜው እንዲህ ያለውን ሕዝብ ቀስቃሽ ንግግር ማድረግን ይመርጣሉ።” ኤርትራ በአፍሪካ ውስጥ የምትዋሰናቸው ሱዳን፣ ኢትዮጵያ እና ጂቡቲ ናቸው። የአካባቢው አገራት በጦርነት ውስጥ ናቸው ካልሆንም ከዚህ ቀደም በተካሄደ ጦርነት ተጎድተዋል። ስለዚህም በቀጠናው ሌላ ጦርነት ከፈነዳ የሚያደርሰው ጉዳት ቀላል አይሆንም። አብዱራህማን እንደሚሉት በአካባቢው አገራት ውስጥ ለጦርነት መቀስቀስ አመቺ ሁኔታዎች አሉ። ስለዚህም በአፍሪካ ቀንድ ሌላ የጦርነት አዙሪት የሚቀሰቀስበት ዕድል ዝግ ነው ለማት እንደማይቻል ያምናሉ። “ሁለቱን አገራት ጨምሮ የአካባቢው መሪዎች ያሉባቸውን አስተዳደራዊ ውድቀቶች እና የሰብአዊ መብቶች ጥሰቶች ለመሻፋፈን ጦርነቶችን ሊቀሰቅሱ ይችላሉ” በማለት የኢሳያስ ስጋት እውን የሚሆንበት አጋጣሚም እንዳለ አመልክተዋል። ለዚህ ደግሞ በዋናነት የሚጠረጥሯቸው ኢትዮጵያን እና ኤርትራን ሲሆን፣ አገራቱ በተለያዩ አጋጣሚዎች የሚያወጧቸው መግለጫዎች እና የግንኙነቶቻቸው መቀዛቀዝ ለዚህ እንደማሳያ ሊወሰዱ እንደሚችሉም ጠቅሰዋል። “ኤርትራ ካሉባት ስጋቶች አንደኛው ውስጣዊ ሲሆን፣ ይህም ዘላቂ መንግሥታዊ ሥርዓት በአግባቡ አለመዘርጋቱ” መሆኑን የሚጠቅሱት አብዱራህማን፣ “ሁለተኛው ደግሞ በኢትዮጵያው ጠቅላይ ሚኒስትር በኩል እየተራመደ ያለው የባሕር በር የማግኘት ጠንካራ ፍላጎት ነው” ይላሉ። የአፍሪካ ቀንድ አካባቢ አገራት በእርስ በርስ ጦርነት አንዳቸው ከሌላኛቸው ጋር በተለያዩ ጊዜያቶች ባደረጓቸው ጦርነቶች የበርካቶች ደም ፈሷል። ሱዳን በእርስ በርስ ጦርነት ውስጥ ስትሆን በኢትዮጵያ ውስጥም ግጭቶች አላባሩም በዚህ ሁኔታ ወደ ሌላ ጦርነት እንዴት ሊገባ ይችላል? “የአካባቢው መሪዎች ውስጣዊ እና ውጫዊ ችግሮቻቸውን ለመፍታት ሰላማዊ መንገድን እንደ መፍትሄ አይወስዱትም። በዚህም ምክንያት እንደ ፕሬዝዳንንት ኢሳያስ ላሉ መሪዎች ሕዝባቸውን በዘላቂ የጦርነት ጥላ ስር እንዲያቆይ ያደርጋል” ይላሉ አብዱራህማን። ሌላኛው የፕሬዝዳንት የስጋት ምንጭ በመካከለኛው ምሥራቅ የተከሰተውን ጦርነት ተከትሎ ምዕራባውያን አገራት በቀይ ባሕር እና በኤደን ባሕረ ሰላጤ ላይ የሚያደርጉት እንቅስቃሴ ነው። ይህም በቀጥታም መሆነ በተዘዋዋሪ ኤርትራን የሚያሳስባት መሆኑን አመልክተዋል። ግዙፍ የንግድ መርከቦች የሚንሳቀሱበት ቀይ ባሕር በጋዛ እስራኤል ጦርነት መቀስቀስ ምክንያት መታወክ አጋጥሞታል። የቀይ ባሕር ተጎራባች በሆነው የአፍሪካ ቀንድ አካባቢም የተለያዩ አገራት ባሕር ኃይሎች ተሰማርተዋል። እስራኤል በጋዛ ላይ የምታካሂደውን ዘመቻ የሚቃወሙት የየመን ሁቲ ታጣቂዎች በንግድ መርከቦች ላይ ተደጋጋሚ ጥቃት ፈጽመዋል። ይህንንም ለመግታት ምዕራባውያን የባሕር እና የአየር ኃይላቸውን በአካባቢው ማሰማራት ከጀመሩ ወራት ተቆጥረዋል። በቀይ ባሕር ላይ ረጅም የባሕር ዳርቻ ያላት ኤርትራም የዚሁ ስጋት ተጋሪ ናት። ከሩሲያ እና ከቻይና ጋር እያጠናከረችው ባለው ግንኙነትም ምዕራባውያን በአካባቢው የሚያደርጉትን እንቅስቃሴ ሚዛን ለማስጠበቅ ለአገራቱ ባሕር ኃይሎች በሯን ልተከፍት ትችላለች። በቅርቡም የሩሲያ ባሕር ኃይል መርከብ በምጽዋ መልህቋን ጥላ እንደነበር ይታወሳል። ፕሬዝዳንቱም በንግግራቸው በአካባቢው የተፈጠረውን ውጥረት በመጠቀም “የበላይነት እና ሁሉን የመቆጣጠር” ኃይሎች “ዓለም አቀፉን የባሕር መስመር ለመጠበቅ” በሚል ሰበብ የሚያካሂዱት “ሕገወጥ ጣልቃ ገብነት እና የባሕር ላይ እሽቅድድም” በአገራቸው ላይ ሊኖረው የሚችለው ተጽእኖ በአግባቡ ሊመረመር ይገባል ብለዋል። ምንም እንኳን የምዕራባውያኑ ኃይሎች በቀይ ባሕር ላይ ያላቸው እንቅስቃሴ በቅርቡ የሰፋ ቢሆንም፣ ኤርትራ በስተደቡብ የምትዋሰናት ጂቡቲ የበርካታ ምዕራባውያንን የባሕር ኃይል የጦር ሰፈሮችን ስታስተናግድ ዘመናት ተቆጥረዋል። ኤርትራ እና ጂቡቲ በድንበር ይገባኛል ከተወዛገቡ በኋላ ያላቸው ግንኙነት በእጅጉ ቀዝቃዛ እና በመጠራጠር የተሞላ ነው። © 2024 BBC. ቢቢሲ ከሌሎች ድረ-ገጾች ለሚመጡ መረጃዎች ሀላፊነት አይወስድም. ስለ ውጪ ሊንኮች ያለን አቀራረብ',\n",
       " 'source': 'www.bbc.com',\n",
       " 'word_count': 1055,\n",
       " 'tags': [],\n",
       " 'language': 'am',\n",
       " 'category': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use kafka to store the data and use faust to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Kafka producer\n",
    "# localhost:9093 is the address of the Kafka broker for faust in docker to work\n",
    "KAFKA_BROKER_URL = os.environ.get('KAFKA_BROKER_URL', 'localhost:9093')\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_BROKER_URL,\n",
    "                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "import faust\n",
    "\n",
    "'''\n",
    "- This is a faust record that will be used to store the scraped data\n",
    "- Faust record, which is a type of data model that Faust can use to understand the structure of your messages.\n",
    "'''\n",
    "class ScrapedData(faust.Record, serializer='json'):\n",
    "    url: str\n",
    "    title: str\n",
    "    source: str\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'url': self.url,\n",
    "            'title': self.title,\n",
    "            'source': self.source,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scrapedData object\n",
    "scraped_data = ScrapedData(url=total_articles[0]['url'], title=total_articles[0]['title'], source=total_articles[0]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x78f5b1547160>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# send the scraped data to the 'scraped_data' Kafka topic\n",
    "producer.send('scraped_data', value=scraped_data.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

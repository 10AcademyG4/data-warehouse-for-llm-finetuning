{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from src.database import MongoDB\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'clean_data'\n",
    "collection_name = 'alain_news_clean'\n",
    "connection_string = 'mongodb://localhost:27017/'\n",
    "clean_db = MongoDB(db_name=db_name, collection_name=collection_name, connection_string=connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_db.remove_duplicates('article_url', 'alain_news_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all documents\n",
    "documents = list(clean_db.collection.find({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split documents into training and test sets\n",
    "train_docs, test_docs = train_test_split(documents, test_size=0.2)  # adjust the test_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(docs, filename):\n",
    "  with open(filename, 'w', encoding='utf8') as f:\n",
    "    for doc in docs:\n",
    "      f.write(doc['title'] + '\\n')\n",
    "      f.write(doc['summary'] + '\\n')\n",
    "      f.write(doc['content'] + '\\n')\n",
    "\n",
    "write_to_file(train_docs, 'amharic_train.txt')\n",
    "write_to_file(test_docs, 'amharic_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=amharic_corpus.txt --model_prefix=m --vocab_size=2000\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: amharic_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: amharic_corpus.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 115500 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=9472503\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9538% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=193\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999538\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 115350 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=4733489\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 176667 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 115350\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 142642\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 142642 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=85196 obj=12.0625 num_tokens=269303 num_tokens/piece=3.16098\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=72587 obj=10.5258 num_tokens=271549 num_tokens/piece=3.74101\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=54434 obj=10.5072 num_tokens=289577 num_tokens/piece=5.31978\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=54398 obj=10.4802 num_tokens=289833 num_tokens/piece=5.32801\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=40798 obj=10.5901 num_tokens=315888 num_tokens/piece=7.74273\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=40796 obj=10.5589 num_tokens=315919 num_tokens/piece=7.74387\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30597 obj=10.7255 num_tokens=343341 num_tokens/piece=11.2214\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=30597 obj=10.6841 num_tokens=343428 num_tokens/piece=11.2242\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22947 obj=10.9031 num_tokens=371248 num_tokens/piece=16.1785\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22947 obj=10.8523 num_tokens=371271 num_tokens/piece=16.1795\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17210 obj=11.1286 num_tokens=398500 num_tokens/piece=23.1551\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17210 obj=11.0677 num_tokens=398481 num_tokens/piece=23.154\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12907 obj=11.3877 num_tokens=425733 num_tokens/piece=32.9847\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12907 obj=11.318 num_tokens=425744 num_tokens/piece=32.9855\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9680 obj=11.6954 num_tokens=451102 num_tokens/piece=46.6014\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9680 obj=11.618 num_tokens=451114 num_tokens/piece=46.6027\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7260 obj=12.0397 num_tokens=476423 num_tokens/piece=65.623\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7260 obj=11.953 num_tokens=476441 num_tokens/piece=65.6255\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5445 obj=12.4252 num_tokens=501285 num_tokens/piece=92.0634\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5445 obj=12.3311 num_tokens=501299 num_tokens/piece=92.0659\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4083 obj=12.848 num_tokens=528472 num_tokens/piece=129.432\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4083 obj=12.7428 num_tokens=528476 num_tokens/piece=129.433\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3062 obj=13.3065 num_tokens=552721 num_tokens/piece=180.51\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3062 obj=13.1959 num_tokens=552725 num_tokens/piece=180.511\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2296 obj=13.8006 num_tokens=577659 num_tokens/piece=251.594\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2296 obj=13.681 num_tokens=577669 num_tokens/piece=251.598\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=13.7669 num_tokens=582502 num_tokens/piece=264.774\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=13.7486 num_tokens=582502 num_tokens/piece=264.774\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# My Amharic corpus is in 'amharic_corpus.txt'\n",
    "spm.SentencePieceTrainer.train('--input=amharic_corpus.txt --model_prefix=m --vocab_size=2000')\n",
    "\n",
    "# 'm.model' and 'm.vocab' files will be created after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the model and use it to tokenize new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁በ', '▁በ', 'አለማቀፍ', '▁ደረጃ', '▁የተ', 'ፈፀመው', '▁የ', 'ሞ', 'ት', '▁ቅጣት']\n",
      "[8, 8, 1615, 312, 45, 1205, 5, 94, 7, 1476]\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')  # replace 'm.model' with the path to your model\n",
    "\n",
    "# Tokenize Amharic text\n",
    "text = \"በ በአለማቀፍ ደረጃ የተፈፀመው የሞት ቅጣት\"\n",
    "\n",
    "# replace with your Amharic text\n",
    "tokens = sp.encode_as_pieces(text)\n",
    "ids = sp.encode_as_ids(text)\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 1615, 312, 45, 1205, 5, 94, 7, 1476]\n"
     ]
    }
   ],
   "source": [
    "# Encode the text\n",
    "encoded_text = sp.encode(text, out_type=int)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained model that has been trained on Amharic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d932d093fdae410493e4df5ce73024b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9c7b599d064f24b9ab9fe90fd7b444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c377866d8e0e4afdb9f940715ee68132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "tokenizer = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assume 'encoded_text' is your tokenized text\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2883\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2881\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2882\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2883\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2941\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2944\u001b[0m     )\n\u001b[1;32m   2946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2950\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# Assume 'encoded_text' is your tokenized text\n",
    "inputs = tokenizer(encoded_text, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
